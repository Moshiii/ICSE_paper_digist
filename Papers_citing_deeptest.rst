DeepXplore: Automated Whitebox Testing of Deep Learning Systems
Kexin et al 2017


white box testing

dataset:
including Udacity self-driving car challenge data, image data
from ImageNet and MNIST, Android malware data from
Drebin, and PDF malware data from Contagio/VirusTotal.

performance: 
The
inputs generated by DeepXplore achieved 34.4% and 33.2%
higher neuron coverage on average than the same number of
randomly picked inputs and adversarial inputs

workflow:

.. image:: img/deepexplore.PNG
   :width: 40pt


DeepGauge: multi-granularity testing criteria for deep learning systems
Lei et al . 2018
whitebox
dataset:
MNIST Imagenet

idea:
 5 types of coverage metrics:
 KMNC NBC SNAC TKNC TKNP

Houdini 
NIPS 2017


-- Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst

DeepConcolic: Testing and Debugging Deep Neural Networks
Youcheng et al 2019

   DeepConcolic is the first tool that implements a concolic testing technique for DNNs

Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing
Jingyi  et al 2019 icse

   In this work, we provide a complementary perspective
   and propose an approach for detecting adversarial samples at
   runtime. The idea is that, given an arbitrary input sample to
   a DNN, to decide at runtime whether it is likely to be an
   adversarial sample or not. If it is, we raise an alarm and report
   that the sample is ‘suspicious’ with certain confidence. Once
   detected, it can be rejected or checked depending on different
   applications. 


Symbolic Execution for Attribution and Attack Synthesis in Neural Networks
Divya et al.  2019 ICSE

   DeepCheck implements techniques for lightweight symbolic
   analysis of DNNs and applies them in the context of image classification to address two challenging problems: 1) identification
   of important pixels (for attribution and adversarial generation);
   and 2) creation of adversarial attacks. 

Formal Security Analysis of Neural Networks using Symbolic Intervals
Shiqi et al 2018 usenix

   In this paper, we present a new direction for formally
   checking security properties of DNNs without using SMT
   solvers. Instead, we leverage interval arithmetic to compute rigorous bounds on the DNN outputs. 

Efficient Formal Safety Analysis of Neural Networks
Shiqi ET AL 2018 NIPS
   
   In this paper, we present a new efficient approach for rigorously checking
   different safety properties of neural networks that significantly outperforms existing
   approaches by multiple orders of magnitude.


Deepmutation: Mutation testing of deep learning systems
lei et al. 2018
   In this paper, we
   propose a mutation testing framework specialized for DL systems
   to measure the quality of test data.


Testing deep neural networks
 Youcheng et al 2019
 
   In this paper, inspired by the MC/DC coverage criterion, we
   propose a family of four novel test criteria that are tailored to structural features
   of DNNs and their semantics.
 
 
DeepRoad: GAN-based metamorphic testing and input validation framework for autonomous driving systems
Mengshi et al 2018

   In this paper, we propose DeepRoad, an unsupervised DNN-based
   framework for automatically testing the consistency of DNN-based
   autonomous driving systems and online validation. 

Tensorfuzz: Debugging neural networks with coverage-guided fuzzing
Augustus et al 2019 

   We introduce testing techniques for neural networks that
   can discover errors occurring only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF)
   methods for neural networks.

Guiding deep learning system testing using surprise adequacy
Jinhan et al 2019

   We propose a novel test
   adequacy criterion for testing of DL systems, called Surprise
   Adequacy for Deep Learning Systems (SADL), which is based
   on the behaviour of DL systems with respect to their training
   data.
   
Simulation-based adversarial test generation for autonomous vehicles with machine learning components
Cumhur et al 2018

   We present a testing framework that
   is compatible with test case generation and automatic falsification methods, which are used to evaluate cyber-physical systems. We demonstrate how the framework can be used to evaluate closed-loop
   properties of an autonomous driving system model that includes the ML components, all within a virtual environment. 

A Quantitative Analysis Framework for Recurrent Neural Network
Xiaoning et al 2019

   In this paper, we
   propose a quantitative analysis framework — DeepStellar—
   to pave the way for effective quality and security analysis of
   software systems powered by RNNs. DeepStellar is generic to
   handle various RNN architectures, including LSTM and GRU,
   scalable to work on industrial-grade RNN models, and extensible
   to develop customized analyzers and tools.

Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects
Michael et al 2019

   In this paper, we present a framework for discovering DNN
   failures that harnesses 3D renderers and 3D models.


Identifying implementation bugs in machine learning based image classifiers using metamorphic testing
Anurag et al 2018

   In this work, we present an articulation of
   the challenges in testing ML based applications. We then present
   our solution approach, based on the concept of Metamorphic Testing, which aims to identify implementation bugs in ML based image
   classifiers. We have developed metamorphic relations for an application based on Support Vector Machine and a Deep Learning based
   application

Towards practical verification of machine learning: The case of computer vision systems
Kexin et al 2017

   In this paper, we propose a generic
   framework for evaluating security and robustness of ML systems
   using different real-world safety properties.

Dlfuzz: Differential fuzzing testing of deep learning systems
Jianmin et al 2018

   In this paper, we propose DLFuzz, the first differential fuzzing
   testing framework to guide DL systems exposing incorrect behaviors.



Testing Untestable Neural Machine Translation: An Industrial Case
Wujie et al 2018

   in this paper, we propose a new approach for
   automatically identifying translation failures, without requiring
   reference translations for a translation task; our approach can
   directly serve as a test oracle for in-vivo testing.


CRADLE: Cross-Backend Validation to Detect and Localize Bugs in Deep Learning Libraries
Hung et al 2019

   Thus, we propose CRADLE, a new approach that focuses on
   finding and localizing bugs in DL software libraries. CRADLE (1)
   performs cross-implementation inconsistency checking to detect
   bugs in DL libraries, and (2) leverages anomaly propagation
   tracking and analysis to localize faulty functions in DL libraries
   that cause the bugs. We evaluate CRADLE on three libraries
   (TensorFlow, CNTK, and Theano)


A Quantitative Analysis Framework for Recurrent Neural Network
Xiaoning et al 2019

  In this paper, we
  propose a quantitative analysis framework — DeepStellar—
  to pave the way for effective quality and security analysis of
  software systems powered by RNNs.

Testing DNN Image Classifiers for Confusion & Bias Errors
Yuchi et al 2019

  We developed a testing technique to automatically detect classbased confusion and bias errors in DNN-driven image classification
  software. 

AsFault: Testing Self-Driving Car Software Using Search-Based Procedural Content Generation
Alessio et al. 2019 

   we developed ASFAULT, a tool for automatically generating
   virtual tests for systematically testing self-driving car software.
   We demonstrate ASFAULT by testing the lane keeping feature
   of an artificial intelligence-based self-driving car software, for
   which ASFAULT generates scenarios that cause it to drive off
   the road.
